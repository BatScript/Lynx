# ğŸ¦‰ Lynx â€” A Directory-Based Contextual Chatbot

Lynx is a local Retrieval-Augmented Generation (RAG) system that reads, parses, and chats with your files â€” all from your local directory.
Itâ€™s privacy-friendly, modular, and built for developers who want fast, contextual insights from their data.


## âš™ï¸ Features
- ğŸ“‚ Directory-based ingestion â€” reads and processes all supported files automatically.
- ğŸ§© Supports multiple formats â€” JSON, PDF, CSV, and HTML.
- ğŸ§  Contextual Q&A â€” chats with your documents using vector-based retrieval.
- âš¡ Caching system â€” powered by SHA-256 hashes to avoid redundant processing.
- ğŸ’¾ MongoDB + Chroma integration â€” for metadata and embeddings storage.
- ğŸ” Similarity search â€” efficient context fetching using Chromaâ€™s built-in methods.


## ğŸ§± Requirements
	â€¢	Python â‰¥ 3.12
	â€¢	Ollama (for local LLMs)
	â€¢	LlamaIndex API key
	â€¢	uv (recommended for dependency management)


## ğŸš€ How to Run the App
1.	Pull the models from Ollama
```
ollama pull qwen3:8b
ollama pull qwen3-embedding:4b
```
2.	Set up the environment
```
cp .env-example .env
```

then fill in your values

3.	Run the app
```
uv run main.py
```

(or use python main.py if uv isnâ€™t installed)



## ğŸ’¡ Tip:
- Make sure INPUT_DIR and VECTORDB_DIR exist before running ingestion.
- Tune CHUNK_SIZE and CHUNK_OVERLAP for optimal accuracy vs. performance.



## ğŸ§© Architecture Overview
1.	File Scanning & Parsing
Lynx scans the directory and parses supported files (.json, .pdf, .csv, .html) into Markdown using custom parsing functions.
2.	Caching & Storage
Each file is hashed (SHA-256). If the hash exists in MongoDB or Chroma, itâ€™s skipped to save time.
Metadata (like file name, hash, and timestamp) is stored in MongoDB.
3.	Chunking & Embeddings
Parsed Markdown is split into chunks and embedded via the embedding model.
Chunks, along with source metadata, are stored in the Chroma vector database.
4.	Chat Retrieval Flow
	â€¢	User asks a question.
	â€¢	Similar chunks are fetched via similarity_search.
	â€¢	Lynx builds a context snippet containing:
	â€¢	File name
	â€¢	Chunk ID
	â€¢	A preview of the chunk text
	â€¢	This context is sent to the chat model along with the userâ€™s query.
	â€¢	The model generates a contextual, file-aware response.



## ğŸ“ˆ Performance Notes
- Ingestion time scales with file size â€” optimization is ongoing.
- Chromaâ€™s persistent vector store allows incremental updates.
- Cached files drastically reduce subsequent processing runs.



## ğŸ§ª Tech Stack
- LLMs: Qwen (via Ollama)
- Vector DB: Chroma
- Database: MongoDB
- Framework: LangChain
- Runtime: Python 3.12 / uv
- Env: .env-based configuration



## ğŸ“š Roadmap
- Add async file ingestion
- Add support for .txt, .docx, .md
- UI for querying files
- REST API endpoints for ingestion & query
- Performance metrics dashboard



## ğŸ§  Example Usage

> User : What are the main points discussed in data.pdf?

> Lynx : The document discusses data ingestion strategies, highlighting schema detection,
data normalization, and the use of streaming pipelines for real-time updates.




## ğŸ§‘â€ğŸ’» Author

Mohit Ranjan
Built with â¤ï¸ and LangChain.